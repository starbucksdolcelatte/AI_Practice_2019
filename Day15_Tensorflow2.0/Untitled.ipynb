{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 25000\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# L2 Regularization 모델와 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델에 L2 Regularization을 적용해보겠습니다.\n",
    "L2_regularization_model = keras.Sequential([\n",
    "    # 각 Dense (Fully Connected) Layer에 인자로 keras.regularizers.l1을 적용해보세요.\n",
    "    # Kernel, bias, activity에 하나씩 적용해보며 결과를 비교해보세요.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,), kernel_regularizer= keras.regularizers.l2(0.0025)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(0.0025)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# L2 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "L2_regularization_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# L2 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "L2_regularization_model.summary()\n",
    "# L2 모델을 학습시킵니다.\n",
    "L2_regularization_history = L2_regularization_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', basic_history),('L2 Regularization', L2_regularization_history)])\n",
    "              \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
